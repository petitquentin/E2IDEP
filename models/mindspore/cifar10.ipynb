{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcd367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from mindspore import context, Tensor, nn, ops\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.train import Model, Callback\n",
    "from mindspore.train.callback import LossMonitor, TimeMonitor, ModelCheckpoint, CheckpointConfig\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "from mindspore.nn import Accuracy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from download import download\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "pgf = False\n",
    "if (pgf):\n",
    "    matplotlib.use(\"pgf\")\n",
    "    matplotlib.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        'font.family': 'serif',\n",
    "        'text.usetex': True,\n",
    "        'pgf.rcfonts': False,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0827a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz\"\n",
    "download(cifar10_url, \"./dataset\", kind=\"tar.gz\", replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e70c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "hidden_units_large = [512, 512]\n",
    "hidden_units_med = [256, 256]\n",
    "hidden_units_low = [64, 64]\n",
    "hidden_units_verylow = [32, 32]\n",
    "\n",
    "epochs = 20\n",
    "nb_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "dropout = [0.25, 0.25, 0.25]\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5243a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(36097:128790985704960,MainProcess):2025-07-30-17:48:49.700.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(36097:128790985704960,MainProcess):2025-07-30-17:48:49.701.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(36097:128790985704960,MainProcess):2025-07-30-17:48:49.701.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(batch_size=64):\n",
    "    # Download CIFAR-10 dataset using MindSpore dataset module\n",
    "    dataset_train = ds.Cifar10Dataset(\"./dataset\", usage=\"train\", shuffle=True)\n",
    "    dataset_test = ds.Cifar10Dataset(\"./dataset\", usage=\"test\", shuffle=False)\n",
    "\n",
    "    trans = [\n",
    "        CV.Rescale(1.0 / 255.0, 0.0),\n",
    "        CV.HWC2CHW()\n",
    "    ]\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    dataset_train = dataset_train.map(operations=trans, input_columns=\"image\")\n",
    "    dataset_train = dataset_train.map(operations=type_cast_op, input_columns=\"label\")\n",
    "    dataset_train = dataset_train.batch(batch_size)\n",
    "\n",
    "    dataset_test = dataset_test.map(operations=trans, input_columns=\"image\")\n",
    "    dataset_test = dataset_test.map(operations=type_cast_op, input_columns=\"label\")\n",
    "    dataset_test = dataset_test.batch(batch_size)\n",
    "\n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "train_ds, test_ds = load_cifar10(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8be30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Cell):\n",
    "    def __init__(self, input_size=input_shape[0] * input_shape[1] * input_shape[2], hidden_sizes=hidden_units_large, num_classes=nb_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Dense(hidden_sizes[1], num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d4907c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten()\n",
      "  (fc1): Dense(input_channels=3072, output_channels=512, has_bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Dense(input_channels=512, output_channels=512, has_bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (out): Dense(input_channels=512, output_channels=10, has_bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid cifar folder, cifar(.bin) files are missing under ./dataset.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/engine/datasetops/source/cifar_op.cc(223).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m time_cb \u001b[38;5;241m=\u001b[39m TimeMonitor()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mloss_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sink_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/train/model.py:1488\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size, initial_epoch)\u001b[0m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m epoch:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel.train\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, the parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must bigger than parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1486\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got the parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1488\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no valid data in dataset, please check dataset file firstly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/dataset/engine/iterators.py:74\u001b[0m, in \u001b[0;36m_cleanup_the_iterators_if_created.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     72\u001b[0m     original_iterators \u001b[38;5;241m=\u001b[39m deepcopy(ITERATORS_LIST)\n\u001b[0;32m---> 74\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# it is used to attribute function like: dataset_size / output_shapes / output_types and\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# it is a GeneratorDataset with two stage pipeline. The first pipeline will create a new iterator\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# which need to be released after dataset_size / output_shapes / output_types end.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# 1. find the iterators which are started by dataset_size / output_shapes / output_types with two stage pipeline\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     iterators_to_be_released \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/dataset/engine/datasets.py:1855\u001b[0m, in \u001b[0;36mDataset.get_dataset_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1854\u001b[0m     runtime_getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_size_getter()\n\u001b[0;32m-> 1855\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[43mruntime_getter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetDatasetSize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1857\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 0 sample from dataset pipeline, check if drop all data or load dataset fail.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid cifar folder, cifar(.bin) files are missing under ./dataset.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/engine/datasetops/source/cifar_op.cc(223).\n\n\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "loss_fn = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=0.001)\n",
    "model = Model(net, loss_fn=loss_fn, optimizer=optimizer, metrics={\"Accuracy\": Accuracy()})\n",
    "print(net)\n",
    "\n",
    "# Callbacks\n",
    "loss_cb = LossMonitor()\n",
    "time_cb = TimeMonitor()\n",
    "\n",
    "# Training\n",
    "model.train(epoch=epochs, train_dataset=train_ds, callbacks=[loss_cb, time_cb], dataset_sink_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53e0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
