{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor, Parameter, context\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import TimeMonitor, LossMonitor\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "from mindspore.nn.optim import Adam\n",
    "from mindspore.communication import init\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "import mindspore.common.dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "pgf = True\n",
    "if (pgf):\n",
    "    matplotlib.use(\"pgf\")\n",
    "    matplotlib.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        'font.family': 'serif',\n",
    "        'text.usetex': True,\n",
    "        'pgf.rcfonts': False,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0827a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random\n",
    "seed = 42\n",
    "reproductibility_mode = True\n",
    "\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n",
    "#context.set_auto_parallel_context(parallel_mode=\"semi_auto_parallel\", enable_alltoall=True, device_num=8, global_rank=0)\n",
    "#init()\n",
    "np.random.seed(seed)\n",
    "ms.set_seed(seed)\n",
    "#context.set_context(save_graphs=True)\n",
    "#context.set_context(save_graph_path=\"./graph_exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log folder\n",
    "log_dir = \"logs/cifar10/\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "print(\"Logs save in:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "input_shape = (32, 32, 3)\n",
    "shape = input_shape[0] * input_shape[1] * input_shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical (one-hot encoding)\n",
    "def to_categorical(y, num_classes=NUM_CLASSES):\n",
    "    \"\"\"Convert labels to one-hot encoding\"\"\"\n",
    "    y_categorical = np.zeros((len(y), num_classes))\n",
    "    for i, label in enumerate(y):\n",
    "        y_categorical[i, label] = 1\n",
    "    return y_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15dc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(batch_size=64, flatten=True):\n",
    "    # Load CIFAR-10 train and test from mindspore dataset\n",
    "    cifar_train = ds.Cifar10Dataset(dataset_dir=\"./cifar-10-batches-bin\", usage=\"train\", shuffle=True)\n",
    "    cifar_test = ds.Cifar10Dataset(dataset_dir=\"./cifar-10-batches-bin\", usage=\"test\", shuffle=False)\n",
    "\n",
    "    # Convert to numpy arrays (images and labels)\n",
    "    x_train, y_train = [], []\n",
    "    for data in cifar_train.create_dict_iterator(output_numpy=True):\n",
    "        img = data[\"image\"]  # (32, 32, 3)\n",
    "        if flatten:\n",
    "            img = img.reshape(-1)  # vecteur 3072\n",
    "        x_train.append(img)\n",
    "        y_train.append(data[\"label\"])\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    for data in cifar_test.create_dict_iterator(output_numpy=True):\n",
    "        img = data[\"image\"]\n",
    "        if flatten:\n",
    "            img = img.reshape(-1)\n",
    "        x_test.append(img)\n",
    "        y_test.append(data[\"label\"])\n",
    "\n",
    "    # Define types\n",
    "    x_train = np.array(x_train, dtype=np.float32)\n",
    "    x_test = np.array(x_test, dtype=np.float32)\n",
    "\n",
    "    # Normaliaation to [0,1]\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0\n",
    "\n",
    "    #Transform to one-hot encoding arrays\n",
    "    y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "    y_dtype = np.float32\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "hidden_units_large = [512, 512]\n",
    "hidden_units_med = [256, 256]\n",
    "hidden_units_low = [64, 64]\n",
    "hidden_units_verylow = [32, 32]\n",
    "\n",
    "epochs = 20\n",
    "dropout = [0.25, 0.25, 0.25]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_cifar10(batch_size = batch_size)\n",
    "\n",
    "print(\"x_train:\", x_train.shape, x_train.dtype)\n",
    "print(\"y_train:\", y_train.shape, y_train.dtype)\n",
    "print(\"x_test:\", x_test.shape, x_test.dtype)\n",
    "print(\"y_test:\", y_test.shape, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset generator\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def create_dataset(x, y, batch_size, shuffle=True):\n",
    "    \"\"\"Create MindSpore dataset from numpy arrays\"\"\"\n",
    "    dataset_generator = DatasetGenerator(x, y, batch_size)\n",
    "    dataset = GeneratorDataset(dataset_generator, [\"data\", \"label\"], shuffle=shuffle)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network model\n",
    "class MLPNet(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_units, dropout_rates, num_classes):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.layers = nn.SequentialCell()\n",
    "        \n",
    "        # First layer\n",
    "        print(\"nn.Dense(input_size, hidden_units[0]) = (\" +str(input_size) + \" \" + str(hidden_units[0]) + \")\")\n",
    "        self.layers.append(nn.Dense(input_size, hidden_units[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        if dropout_rates[0] > 0:\n",
    "            self.layers.append(nn.Dropout(keep_prob = 1 - dropout_rates[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, len(hidden_units)):\n",
    "            print(\"nn.Dense(hidden_units[i-1], hidden_units[i] with i=\" + str(i) + \") = (\" +str(hidden_units[i-1]) + \" \" + str(hidden_units[i]) + \")\")\n",
    "            self.layers.append(nn.Dense(hidden_units[i-1], hidden_units[i]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            if dropout_rates[i] > 0:\n",
    "                self.layers.append(nn.Dropout(keep_prob = 1 - dropout_rates[i]))\n",
    "        \n",
    "        # Output layer\n",
    "        print(\"nn.Dense(hidden_units[-1], num_classes) = (\" +str(hidden_units[-1]) + \" \" + str(num_classes) + \")\")\n",
    "        self.layers.append(nn.Dense(hidden_units[-1], num_classes))\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = x.astype(mstype.float32)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_and_evaluate_model(x_train, y_train, x_test, y_test, hidden_units_config, model_name):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    input_size = x_train.shape[1]\n",
    "    model = MLPNet(input_size, hidden_units_config, dropout, NUM_CLASSES)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss_fn = SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')\n",
    "    optimizer = Adam(model.trainable_params(), learning_rate=0.001)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = create_dataset(x_train, y_train, batch_size, shuffle=True)\n",
    "    test_dataset = create_dataset(x_test, y_test, batch_size, shuffle=False)\n",
    "    \n",
    "    # Define metrics\n",
    "    accuracy = Accuracy()\n",
    "    \n",
    "    # Create model wrapper\n",
    "    model_wrapper = Model(model, loss_fn, optimizer, metrics={'accuracy': accuracy})\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    model_wrapper.train(epochs, train_dataset, callbacks=[TimeMonitor(), LossMonitor()])\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"---- {train_time} seconds ----\")\n",
    "    \n",
    "    # Evaluation\n",
    "    start_time = time.time()\n",
    "    eval_result = model_wrapper.eval(test_dataset)\n",
    "    eval_time = time.time() - start_time\n",
    "    print(f\"---- {eval_time} seconds ----\")\n",
    "    \n",
    "    test_accuracy = eval_result['accuracy']\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Save model\n",
    "    ms.save_checkpoint(model, os.path.join(log_dir, f\"{model_name}.ckpt\"))\n",
    "    \n",
    "    return test_accuracy, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_time_train = []\n",
    "\n",
    "# Train different model sizes\n",
    "print(\"Training Standard (Large) Model\")\n",
    "model_large_acc, train_time = train_and_evaluate_model(\n",
    "    x_train, y_train, x_test, y_test, hidden_units_large, \"model_large\")\n",
    "base_time_train.append(train_time)\n",
    "\n",
    "print(\"Training Medium Model\")\n",
    "model_med_acc, train_time = train_and_evaluate_model(\n",
    "    x_train, y_train, x_test, y_test, hidden_units_med, \"model_med\")\n",
    "base_time_train.append(train_time)\n",
    "\n",
    "print(\"Training Low Model\")\n",
    "model_low_acc, train_time = train_and_evaluate_model(\n",
    "    x_train, y_train, x_test, y_test, hidden_units_low, \"model_low\")\n",
    "base_time_train.append(train_time)\n",
    "\n",
    "print(\"Training Very Low Model\")\n",
    "model_verylow_acc, train_time = train_and_evaluate_model(\n",
    "    x_train, y_train, x_test, y_test, hidden_units_verylow, \"model_verylow\")\n",
    "base_time_train.append(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff08ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMTX(input_path):\n",
    "    is_init = False\n",
    "    count = 0\n",
    "    expected = 0\n",
    "    with open(input_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            list_line = line.split()\n",
    "            if list_line[0].isdigit():\n",
    "                if(not is_init):\n",
    "                    is_init = True\n",
    "                    ev = np.zeros((int(list_line[0]), int(list_line[1])))\n",
    "                    expected = int(list_line[2])\n",
    "                else:\n",
    "                    ev[int(list_line[0]) - 1][int(list_line[1]) - 1] = float(list_line[2])\n",
    "                    count = count + 1\n",
    "                \n",
    "    if(expected != count):\n",
    "        print(\"Err: Not the same nnz between expected and find \", str(count), \"/\", str(expected))\n",
    "    return ev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6878ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EV_EXPLORE = 50\n",
    "\n",
    "acc_test_large = []\n",
    "acc_test_med = []\n",
    "acc_test_low = []\n",
    "acc_test_verylow = []\n",
    "emb_time_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MAX_EV_EXPLORE):\n",
    "    path_ev = \"../../data/output/cifar10/eigen_\" + str(i + 1) + \".mtx\"\n",
    "    try:\n",
    "        ev = readMTX(path_ev)\n",
    "        print(f\"Eigenvector {i+1} shape: {ev.shape}\")\n",
    "        \n",
    "        # Apply embedding transformation\n",
    "        x_train_emb = np.matmul(x_train, ev).astype(np.float32)\n",
    "        x_test_emb = np.matmul(x_test, ev).astype(np.float32)\n",
    "        \n",
    "        # Train models with embeddings\n",
    "        print(f\"Training embedding {i+1} - Large model\")\n",
    "        acc_emb, train_time = train_and_evaluate_model(\n",
    "            x_train_emb, y_train, x_test_emb, y_test, hidden_units_large, f\"emb_{i}_model_large\")\n",
    "        acc_test_large.append(acc_emb)\n",
    "        emb_time_train.append(train_time)\n",
    "        \n",
    "        print(f\"Training embedding {i+1} - Medium model\")\n",
    "        acc_emb, train_time = train_and_evaluate_model(\n",
    "            x_train_emb, y_train, x_test_emb, y_test, hidden_units_med, f\"emb_{i}_model_med\")\n",
    "        acc_test_med.append(acc_emb)\n",
    "        emb_time_train.append(train_time)\n",
    "        \n",
    "        print(f\"Training embedding {i+1} - Low model\")\n",
    "        acc_emb, train_time = train_and_evaluate_model(\n",
    "            x_train_emb, y_train, x_test_emb, y_test, hidden_units_low, f\"emb_{i}_model_low\")\n",
    "        acc_test_low.append(acc_emb)\n",
    "        emb_time_train.append(train_time)\n",
    "        \n",
    "        print(f\"Training embedding {i+1} - Very Low model\")\n",
    "        acc_emb, train_time = train_and_evaluate_model(\n",
    "            x_train_emb, y_train, x_test_emb, y_test, hidden_units_verylow, f\"emb_{i}_model_verylow\")\n",
    "        acc_test_verylow.append(acc_emb)\n",
    "        emb_time_train.append(train_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing eigenvector {i+1}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41489f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Large model accuracies:\", acc_test_large)\n",
    "print(\"Medium model accuracies:\", acc_test_med)\n",
    "print(\"Low model accuracies:\", acc_test_low)\n",
    "print(\"Very low model accuracies:\", acc_test_verylow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e315e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "\n",
    "with open(os.path.join(log_dir, \"emb_acc.pkl\"), \"wb\") as f:\n",
    "    pickle.dump([acc_test_large, acc_test_med, acc_test_low, acc_test_verylow], f)\n",
    "\n",
    "with open(os.path.join(log_dir, \"base_acc.pkl\"), \"wb\") as f:\n",
    "    pickle.dump([model_large_acc, model_med_acc, model_low_acc, model_verylow_acc], f)\n",
    "\n",
    "with open(os.path.join(log_dir, \"train_time.pkl\"), \"wb\") as f:\n",
    "    pickle.dump([base_time_train, emb_time_train], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
